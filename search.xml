<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[在XGBoost中通过Early Stop避免过拟合]]></title>
    <url>%2F2018%2F12%2F15%2Fearly-stop-in-xgboost%2F</url>
    <content type="text"><![CDATA[本文翻译自Avoid Overfitting By Early Stopping With XGBoost In Python，讲述如何在使用XGBoost建模时通过Early Stop手段来避免过拟合。全文系作者原创，仅供学习参考使用，转载授权请私信联系，否则将视为侵权行为。码字不易，感谢支持。以下为全文内容： 过拟合问题是在使用复杂的非线性学习算法时会经常碰到，比如gradient boosting算法。 在这篇博客中你将发现如何通过Early Stop方法使得我们在使用Python中的XGBoost模型时可以尽可能地避免过拟合问题。 读完这篇博客后，你将学到： Early Stop可以减少训练集上的过拟合 在使用XGBoost模型时如何监控训练过程中模型的表现，如何绘制学习曲线 如何使用Early Stop方法在模型表现最好的时候停止训练 让我们开始吧。 使用Early Stop避免过拟合Early Stop是训练复杂机器学习模型以避免其过拟合的一种方法。 它通过监控模型在一个额外的测试集上的表现来工作，当模型在测试集上的表现在连续的若干次（提前指定好的）迭代中都不再提升时它将终止训练过程。 它通过尝试自动选择拐点来避免过拟合，在拐点处，测试数据集的性能开始下降，而训练数据集的性能随着模型开始过拟合而继续改善。 性能的度量可以是训练模型时正在使用的损失函数（例如对数损失），或通常意义上用户感兴趣的外部度量（例如分类精度）。 在XGBoost中监控模型的表现XGBoost模型在训练时可以计算并输入在某个指定的测试数据集的性能表现。 在调用model.fit()函数时，可以指定测试数据集和评价指标，同时设置verbose参数为True，这样就可以在训练过程中输出模型在测试集的表现。 例如，我们可以通过下面的方法在使用XGBoost训练二分类任务时输出分类错误率（通过“error”指定）： 12eval_set = [(X_test, y_test)]model.fit(X_train, y_train, eval_metric="error", eval_set=eval_set, verbose=True) XGBoost提供了一系列的模型评价指标，包括但不限于： “rmse” 代表均方根误差 “mae” 代表平均绝对误差 “logloss” 代表二元对数损失 “mlogloss” 代表m-元对数损失 “error” 代表分类错误率 “auc” 代表ROC曲线下面积 完整的列表见XGBoost文档中的“Learning Task Parameters””章节。 例如，我们可以演示如何监控使用UCI机器学习存储库（更新：从这里下载）的关于Pima糖尿病发病数据集的XGBoost模型在训练过程中的性能指标。 完整代码清单如下： 12345678910111213141516171819202122# monitor training performancefrom numpy import loadtxtfrom xgboost import XGBClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_score# load datadataset = loadtxt('pima-indians-diabetes.csv', delimiter=",")# split data into X and yX = dataset[:,0:8]Y = dataset[:,8]# split data into train and test setsX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)# fit model no training datamodel = XGBClassifier()eval_set = [(X_test, y_test)]model.fit(X_train, y_train, eval_metric="error", eval_set=eval_set, verbose=True)# make predictions for test datay_pred = model.predict(X_test)predictions = [round(value) for value in y_pred]# evaluate predictionsaccuracy = accuracy_score(y_test, predictions)print("Accuracy: %.2f%%" % (accuracy * 100.0)) 运行这段代码将会在67%的数据集上训练模型，并且在每一轮迭代中使用剩下的33%数据来评估模型的性能。 每次迭代都会输出分类错误，最终将会输出最后的分类准确率。 12345678910111213...[89] validation_0-error:0.204724[90] validation_0-error:0.208661[91] validation_0-error:0.208661[92] validation_0-error:0.208661[93] validation_0-error:0.208661[94] validation_0-error:0.208661[95] validation_0-error:0.212598[96] validation_0-error:0.204724[97] validation_0-error:0.212598[98] validation_0-error:0.216535[99] validation_0-error:0.220472Accuracy: 77.95% 观察所有的输出，我们可以看到，在训练快要结束时测试集上的模型性能的变化是平缓的，甚至变得更差。 使用学习曲线来评估XGBoost模型我们可以提取出模型在测试数据集上的表现并绘制成图案，从而更好地洞察到在整个训练过程中学习曲线是如何变化的。 在调用XGBoost模型时我们提供了一个数组，数组的每一项是一个X和y的配对。在测试集之外，我们同时将训练集也作为输入，从而观察在训练过程中模型在训练集和测试集上各自的表现。 例如： 12eval_set = [(X_train, y_train), (X_test, y_test)]model.fit(X_train, y_train, eval_metric="error", eval_set=eval_set, verbose=True) 模型在各个数据集上的表现可以在训练结束后通过model.evals_result()函数获取，这个函数返回一个dict包含了评估数据集的代码和对应的分数列表，例如： 12results = model.evals_result()print(results) 这将输出如下的结果：1234&#123; 'validation_0': &#123;'error': [0.259843, 0.26378, 0.26378, ...]&#125;, 'validation_1': &#123;'error': [0.22179, 0.202335, 0.196498, ...]&#125;&#125; “validation_0”和“validation_1”代表了在调用fit()函数时传给eval_set参数的数组中数据集的顺序。 一个特定的结果，比如第一个数据集上的分类错误率，可以通过如下方法获取： 1results['validation_0']['error'] 另外我们可以指定更多的评价指标，从而同时获取多种评价指标的变化情况。 接着我们可以使用收集到的数据绘制曲线，从而更直观地了解在整个训练过程中模型在训练集和测试集上的表现究竟如何。 下面是一段完整的代码，展示了如何将收集到的数据绘制成学习曲线： 12345678910111213141516171819202122232425262728293031323334353637383940414243# plot learning curvefrom numpy import loadtxtfrom xgboost import XGBClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom matplotlib import pyplot# load datadataset = loadtxt('pima-indians-diabetes.csv', delimiter=",")# split data into X and yX = dataset[:,0:8]Y = dataset[:,8]# split data into train and test setsX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)# fit model no training datamodel = XGBClassifier()eval_set = [(X_train, y_train), (X_test, y_test)]model.fit(X_train, y_train, eval_metric=["error", "logloss"], eval_set=eval_set, verbose=True)# make predictions for test datay_pred = model.predict(X_test)predictions = [round(value) for value in y_pred]# evaluate predictionsaccuracy = accuracy_score(y_test, predictions)print("Accuracy: %.2f%%" % (accuracy * 100.0))# retrieve performance metricsresults = model.evals_result()epochs = len(results['validation_0']['error'])x_axis = range(0, epochs)# plot log lossfig, ax = pyplot.subplots()ax.plot(x_axis, results['validation_0']['logloss'], label='Train')ax.plot(x_axis, results['validation_1']['logloss'], label='Test')ax.legend()pyplot.ylabel('Log Loss')pyplot.title('XGBoost Log Loss')pyplot.show()# plot classification errorfig, ax = pyplot.subplots()ax.plot(x_axis, results['validation_0']['error'], label='Train')ax.plot(x_axis, results['validation_1']['error'], label='Test')ax.legend()pyplot.ylabel('Classification Error')pyplot.title('XGBoost Classification Error')pyplot.show() 运行这段代码将会在每一次训练迭代中输出模型在训练集和测试集上的分类错误率。我们可以通过设置verbose=False来关闭输出。 我们绘制了两张图，第一张图表示的是模型在每一轮迭代中在两个数据集上的对数损失： 第二张图表示分类错误率： 从第一张图来看，似乎有机会可以进行Early Stop，大约在20到40轮迭代时比较合适。 从第二张图可以得到相似的结果，大概在40轮迭代时效果比较理想。 在XGBoost中进行Early StopXGBoost提供了在指定轮数完成后提前停止训练的功能。 除了提供用于评估每轮迭代中的评价指标和数据集之外，还需要指定一个窗口大小，意味着连续这么多轮迭代中模型的效果没有提升。这是通过early_stopping_rounds参数来设置的。 例如，我们可以像下面这样设置连续10轮中对数损失都没有提升： 12eval_set = [(X_test, y_test)]model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric="logloss", eval_set=eval_set, verbose=True) 如果同时指定了多个评估数据集和多个评价指标，early_stopping_rounds将会使用数组中的最后一个作为依据。 下面提供了一个使用early_stopping_rounds的详细例子： 123456789101112131415161718192021222324# early stoppingfrom numpy import loadtxtfrom xgboost import XGBClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_score# load datadataset = loadtxt('pima-indians-diabetes.csv', delimiter=",")# split data into X and yX = dataset[:,0:8]Y = dataset[:,8]# split data into train and test setsseed = 7test_size = 0.33X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)# fit model no training datamodel = XGBClassifier()eval_set = [(X_test, y_test)]model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric="logloss", eval_set=eval_set, verbose=True)# make predictions for test datay_pred = model.predict(X_test)predictions = [round(value) for value in y_pred]# evaluate predictionsaccuracy = accuracy_score(y_test, predictions)print("Accuracy: %.2f%%" % (accuracy * 100.0)) 运行这段代码将得到如下的输出（部分）：1234567891011...[35] validation_0-logloss:0.487962[36] validation_0-logloss:0.488218[37] validation_0-logloss:0.489582[38] validation_0-logloss:0.489334[39] validation_0-logloss:0.490969[40] validation_0-logloss:0.48978[41] validation_0-logloss:0.490704[42] validation_0-logloss:0.492369Stopping. Best iteration:[32] validation_0-logloss:0.487297 我们可以看到模型在迭代到42轮时停止了训练，在32轮迭代后观察到了最好的效果。 通常将early_stopping_rounds设置为一个与总训练轮数相关的函数（本例中是10%），或者通过观察学习曲线来设置使得训练过程包含拐点，这两种方法都是不错的选择。 总结在这篇博客中你发现了如何监控模型的表现以及怎么做Early Stop。 你学会了： 使用Early Stop手段在模型过拟合之前停止训练 在使用XGBoost模型时如何监控模型的表现并绘制出模型的学习曲线 在训练XGBoost模型时如何设置Early Stop参数 关于Early Stop或者这篇博客你还有什么想问的问题吗？欢迎在下方的评论区留言，我将尽我最大的努力来解答。 以上就是本文的全部内容，如果您喜欢这篇文章，欢迎将它分享给朋友们。 感谢您的阅读，祝您生活愉快！ 作者：小美哥2018-12-15]]></content>
      <tags>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用pytest进行Python自动化测试]]></title>
    <url>%2F2018%2F06%2F17%2Fpytest-basic-usage%2F</url>
    <content type="text"><![CDATA[简介与安装pytest是一个针对Python的自动化测试框架，它可以使简单的和可扩展的测试变得容易。使用pytest进行测试是简洁和可读的，不需要样板代码，这使得我们可以很方便地进行自动化测试。pytest是一个全平台通用的工具，支持的Python版本包括Python 2.7, 3.4, 3.5, 3.6, Jython, PyPy-2.3 和通常的Python第三方模块一样，pytest可以直接通过pip安装1$ pip install -U pytest 安装完成后可以运行pytest命令来查看当前版本12$ pytest --versionThis is pytest version 3.6.1, imported from /usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pytest.py 基本用法我们先来创建一个以test_开头的测试函数123456# content of test_sample.pydef func(x): return x + 1def test_answer(): assert func(3) == 5 然后在当前目录下运行pytest命令123456789101112131415161718$ pytest=========================== test session starts ============================platform linux -- Python 3.x.y, pytest-3.x.y, py-1.x.y, pluggy-0.x.yrootdir: $REGENDOC_TMPDIR, inifile:collected 1 itemtest_sample.py F [100%]================================= FAILURES =================================_______________________________ test_answer ________________________________ def test_answer():&gt; assert func(3) == 5E assert 4 == 5E + where 4 = func(3)test_sample.py:5: AssertionError========================= 1 failed in 0.12 seconds ========================= 可以看到，pytest自动地运行了test_answer函数，同时提示测试失败，因为func函数的返回值是4，而不是5，所以assert语句报错。 如果想要同时运行多个测试也很简单，在此之前我们需要先了解下pytest是如何确定需要执行哪些测试的。实际上，pytest遵循了标准的测试发现守则，具体来说有如下几条： 如果没有指定参数，则从testpaths（如果配置）或当前目录开始。或者可以在命令行参数中使用目录、文件名或节点ID的任何组合。 递归地进入文件夹，除非他们被指定为norecursedirs 在这些目录中，寻找形如test_*.py或者*_test.py的文件 在这些文件中，收集如下的这些项目： 以test_开头的全局函数 以Test开头的类中以test_开头的函数 除了上面的规则之外，我们也可以配置个性化的测试发现守则 在PyCharm中配置pytest通常我们会使用IDE进行日常的开发工作，PyCharm是一款针对Python的IDE，深受广大Python爱好者的青睐。我们可以在PyCharm中配置pytest，从而可以直接在PyCharm中使用pytest的功能，方便且高效。 当我们在命令行安装好pytest后，在PyCharm中进入 File | Settings（在Mac OS上是Preferences）| Tools | Python Integrated Tools ，在右边的窗口中有一个 Default test runner 选项，点击下拉框可以看到py.test，选择该项后点击Apply即可。然后我们可以回到写有测试函数的文件中直接右键，会出现一个Run &#39;py.test&#39; for project_name，直接点击即可运行自动化测试。 当然，以上只是pytest基本的用法，还有很多高级的技巧能够进一步简化测试的过程，提高测试的质量，具体的可以到官方文档上查看。 参考pytest官方文档python单元测试框架pytest简介 以上就是本文的全部内容，如果您喜欢这篇文章，欢迎将它分享给朋友们。 全文系作者原创，仅供学习参考使用，转载授权请私信联系，否则将视为侵权行为。码字不易，感谢支持。 感谢您的阅读，祝您生活愉快！ 作者：小美哥2018-06-17]]></content>
      <categories>
        <category>tool</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二分类模型常用评价指标汇总]]></title>
    <url>%2F2018%2F06%2F09%2Fmachine-learning-matrix%2F</url>
    <content type="text"><![CDATA[有很多指标可以衡量机器学习模型的效果，不同的任务使用的评价指标也不尽相同。本文对二分类任务的评价指标加以总结。全文系作者原创，仅供学习参考使用，转载授权请私信联系，否则将视为侵权行为。码字不易，感谢支持。 在二分类问题中，数据的标签通常用（0/1）来表示，在模型训练完成后进行测试时，会对测试集的每个样本计算一个介于0~1之间的概率，表征模型认为该样本为阳性的概率，我们可以选定一个阈值，将模型计算出的概率进行二值化，比如选定阈值=0.5，那么当模型输出的值大于等于0.5时，我们就认为模型将该样本预测为阳性，也就是标签为1，反之亦然。选定的阈值不同，模型预测的结果也会相应地改变。二元分类模型的单个样本预测有四种结果： 真阳性（TP）：判断为阳性，实际也是阳性。 伪阳性（FP）：判断为阳性，实际却是阴性。 真阴性（TN）：判断为阴性，实际也是阴性。 伪阴性（FN）：判断为阴性，实际却是阳性。 这四种结果可以画成2 × 2的混淆矩阵： 有了混淆矩阵，就可以定义各种指标了。 TPR（真阳性率） = TP / (TP + FN) FPR（假阳性率） = FP / (FP + TN) Accuracy（准确率）= (TP + TN) / (TP+TN+FP+FN) Precision（精确率）= TP / (TP + FP) Recall（召回率）= TP / (TP + FN) F1值 = 2TP / (2TP+FP+FN) PPV（positive predictive value）= TP / (TP + FP) sensitivity（敏感性）= TP / (TP + FN) specificity（特异性）= TN / (TN + FP) 以上就是本文的全部内容，如果您喜欢这篇文章，欢迎将它分享给朋友们。 感谢您的阅读，祝您生活愉快！ 作者：小美哥2018-06-12]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用PyCharm进行Python远程开发]]></title>
    <url>%2F2018%2F05%2F20%2Fpycharm-remote-development%2F</url>
    <content type="text"><![CDATA[本文翻译自 Feature Spotlight: Python remote development with PyCharm，讲述如何使用PyCharm进行Python远程开发。全文系作者原创，仅供学习参考使用，转载授权请私信联系，否则将视为侵权行为。码字不易，感谢支持。以下为全文内容： 这篇博客创作于2015年。如果你正在使用PyCharm 2018.1或者更新的版本，请参考这篇博客 各位朋友大家周五好， 在今天的博客中我将介绍PyCharm中的一些基本原则和特性，这些特点使得我们可以很容易地进行Python远程开发。为了解释和说明这些特性，我将使用一个非常简单的 flask web application 作为例子。尽情享受吧！ 注意：远程开发的特性只针对PyCharm专业版提供。如果想了解更多这方便的信息可以查看 editions comparison page 首先我从 https://github.com/mitsuhiko/flask 克隆flask仓库，然后我在PyCharm的欢迎界面打开了blueprintexample目录，这个目录下存储了后面我将用来演示的 flask application 的源码。 PyCharm打开了这个目录并且基于该目录创建了一个项目： 现在我准备开始设置远程机器来启动远程开发了。我使用了在PyCharm中提供良好支持的Vagrant。在我之前的一篇博客中我已经涵盖了Vagrant的集成，所以这里提供的仅仅是运行一台VM的直接步骤。我在项目的根目录进入 Tools | Vagrant | Init 然后选择了事先从 collection of vagrant boxes 加载好的Ubuntu 14.04的镜像，这个操作会在项目根目录下创建Vagrantfile。现在，我将编辑这个文件来配置一个专用网络，使我的VM从我的主机上可见： 接着，我将使用 Tools | Vagrant | Up 来启动VM，PyCharm会向我展示VM已经启动并且正在运行： 我们可以在PyCharm中打开一个本地的终端来测试VM： 可以看到，VM响应了ping请求。现在，我想在VM上运行我的Web应用程序，所以我需要将我的项目源代码复制到远程主机上。在PyCharm内部部署工具中很容易做到这一点。 我转到 Tools | Deployment | Configuration 并指定了连接VM的参数： 在同一个窗口下的 Mappings 标签栏中我指定了路径的映射规则： 在这个例子中我希望我当前的本地目录 blueprintexampe 被映射到远端的 /home/vagrant/blueprintremote 现在我可以在项目视图中右键单击我的项目，并选择 Upload to ： 这将把我的项目上传到远程机器上的指定目录： 最简单的特点之一是，您可以通过单击 Tools | Deployment | Automatic Upload 来设置自动上传： 当完成了这个设置后，本地的所有更改将自动上传到远程机器，因此您不必担心远程主机上的代码更新。很酷，不是吗？ 所以现在，我要修改我的项目中的一个文件，使得 flask application 能够在远端可见（添加 host= ‘0.0.0.0’ 作为一个参数到 app.run() ），PyCharm会将本地的变化自动上传到远程机器： 接下来，我将指定项目所使用的Python解释器。我进入 File | Settings（在Mac OS上是Preferences）| Project | Project Interpreter。默认情况下，PyCharm将本地Python解释器设置为项目解释器，因此我将其更改为远程解释器： 由于我已经创建了部署配置，PyCharm将开始从现有部署配置导出Python解释器设置： 但我也可以手动指定远程解释器，使用SSH凭据或Vagrant配置。在这里我将手动完成： 在指定了新的远程Python解释器之后，PyCharm开始索引，并发现在项目解释器上没有安装flask模块： 我可以通过在被红色突出显示的未解决的参考错误上使用 ALT+Enter 轻松解决这个问题： 好吧。现在一切正常，所以我们最终可以指定 Run | Debug 配置并启动我们的应用程序。让我们进入 Run | Edit Configurations 并添加一个新的Python Run | Debug配置： 在 Run | Debug 配置对话框中，我指定新配置的名称和要在远程主机上执行的脚本。Python默认设置了这个新的运行配置的项目解释器（在这种情况下为远程），最后我需要为这个特定的运行配置指定路径映射： 看起来我们都准备好了。我点击运行按钮： PyCharm显示应用程序在VM上的端口5000上运行并运行。我打开浏览器检查应用程序是否真的工作： 从现在开始，我们可以像通常的本地项目一样使用这个项目。PyCharm负责上传任何本地的更改到远程机器并保持VM运行。 使用相同的 Run | Debug 配置，我们可以做一个简单的远程调试会话，在编辑器中放置几个断点： 单击“调试”按钮或转到“运行调试”： 以上就是全部的内容！希望您能理解PyCharm的远程开发功能，它能够使得Python远程开发成为轻而易举的事。 如果您仍渴望了解PyCharm远程开发能力以及其他远程开发功能的详细信息，请参阅联机帮助。 以上就是本文的全部内容，如果你喜欢这篇文章，欢迎将它分享给朋友们。 感谢您的阅读，祝您生活愉快！ 作者：小美哥2018-05-20]]></content>
      <categories>
        <category>tool</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gensim计算文档相似度]]></title>
    <url>%2F2018%2F04%2F23%2Fgensim-doc-similarity%2F</url>
    <content type="text"><![CDATA[gensim是一个基于Python语言的开源工具集，用于处理向量空间模型(vector space modeling)和话题模型(topic modeling)的相关问题。本文分享如何使用gensim工具来计算两篇中文文档的相似度。 首先我们要生成一些中文文档。下面的代码生成一个名为documents的文档列表，由于是演示，这里的每个文档中只有几个词语。关于如何对原始的字符串做分词操作，可以参考jieba 1234567891011from gensim import corporafrom pprint import pprintdocuments = ["你好 好的 不错 笨蛋", "笨蛋 傻瓜 傻子 哈哈", "好的 不错 你好 哈哈", "有趣 可以 好的 不错 还行", "傻瓜 傻子 二货 还行", "可以 好的 不错 哈哈", "有趣 有趣 哈哈 哈哈"]texts = [[word for word in document.split()] for document in documents]pprint(texts) [[&apos;你好&apos;, &apos;好的&apos;, &apos;不错&apos;, &apos;笨蛋&apos;], [&apos;笨蛋&apos;, &apos;傻瓜&apos;, &apos;傻子&apos;, &apos;哈哈&apos;], [&apos;好的&apos;, &apos;不错&apos;, &apos;你好&apos;, &apos;哈哈&apos;], [&apos;有趣&apos;, &apos;可以&apos;, &apos;好的&apos;, &apos;不错&apos;, &apos;还行&apos;], [&apos;傻瓜&apos;, &apos;傻子&apos;, &apos;二货&apos;, &apos;还行&apos;], [&apos;可以&apos;, &apos;好的&apos;, &apos;不错&apos;, &apos;哈哈&apos;], [&apos;有趣&apos;, &apos;有趣&apos;, &apos;哈哈&apos;, &apos;哈哈&apos;]] 然后我们要对原始的文档做一些预处理。这里我们统计了每个词语出现的次数，移除了只出现一次的词语，因为这些词语通常不会对计算文档相似度产生任何贡献。其他的预处理操作还包括移除数字、字母以及标点符号等，这里我们没有展示，如果实际中遇到可以进行相应的处理。 1234567from collections import defaultdictfrequency = defaultdict(int)for text in texts: for token in text: frequency[token] += 1texts = [[token for token in text if frequency[token] &gt; 1] for text in texts]pprint(texts) [[&apos;你好&apos;, &apos;好的&apos;, &apos;不错&apos;, &apos;笨蛋&apos;], [&apos;笨蛋&apos;, &apos;傻瓜&apos;, &apos;傻子&apos;, &apos;哈哈&apos;], [&apos;好的&apos;, &apos;不错&apos;, &apos;你好&apos;, &apos;哈哈&apos;], [&apos;有趣&apos;, &apos;可以&apos;, &apos;好的&apos;, &apos;不错&apos;, &apos;还行&apos;], [&apos;傻瓜&apos;, &apos;傻子&apos;, &apos;还行&apos;], [&apos;可以&apos;, &apos;好的&apos;, &apos;不错&apos;, &apos;哈哈&apos;], [&apos;有趣&apos;, &apos;有趣&apos;, &apos;哈哈&apos;, &apos;哈哈&apos;]] 接着我们生成了包含所有词语的词典，为后面的计算做准备。在实际情况中，词典通常会很大，为了避免重复计算，可以将词典保存下来。 123dictionary = corpora.Dictionary(texts)dictionary.save('sample.dict')print(dictionary) Dictionary(10 unique tokens: [&apos;不错&apos;, &apos;你好&apos;, &apos;好的&apos;, &apos;笨蛋&apos;, &apos;傻子&apos;]...) 有了前一步生成的词典，我们就可以将每个词语转化成一个索引，表示该词语在所有文档中出现的次序，然后每个文档就可以转化成一个索引的列表，比如这里的第一个文档表示为[(0, 1), (1, 1), (2, 1), (3, 1)]，它的含义是在第一篇文档中，第0个词语(也就是“你好”)出现了一次，第一个词语(也就是“好的”)出现了一次，依次类推。 123corpus = [dictionary.doc2bow(text) for text in texts]corpora.MmCorpus.serialize('sample.mm', corpus)pprint(corpus) [[(0, 1), (1, 1), (2, 1), (3, 1)], [(3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (1, 1), (2, 1), (6, 1)], [(0, 1), (2, 1), (7, 1), (8, 1), (9, 1)], [(4, 1), (5, 1), (9, 1)], [(0, 1), (2, 1), (6, 1), (7, 1)], [(6, 2), (8, 2)]] 然后我们可以调用gensim中的tf-idf模块来进一步对每个单词计算权重。关于tf-idf的计算方法，可以参考维基百科 1234from gensim import models, similaritiestf_idf = models.TfidfModel(corpus)vec = [(0, 1), (5, 1), (7, 1)]print(tf_idf[vec]) [(0, 0.3011997233053068), (5, 0.6742695034927825), (7, 0.6742695034927825)] 现在，我们可以开始计算两个文档的相似度了。这里我们采用了余弦相似度作为衡量指标，当然还有其他的方式，可以参考文本相似度计算方法研究综述 123index = similarities.SparseMatrixSimilarity(tf_idf[corpus], num_features=10)sims = index[tf_idf[vec]]print(sims) [0.08686648 0.37695488 0.10641449 0.43870124 0.38928968 0.63969857 0. ] 我们还可以将整个相似度矩阵打印出来，就可以看到前面我们所准备的七篇文档两两之间的相似度。1print(index[tf_idf[corpus]]) [[1. 0.3609379 0.71441036 0.13975498 0. 0.20378576 0. ] [0.3609379 1.0000001 0.08823138 0. 0.64554304 0.08823138 0.10185669] [0.71441036 0.08823138 1.0000001 0.17120475 0. 0.37446705 0.1440983 ] [0.13975498 0. 0.17120475 1. 0.31315398 0.60019135 0.4952337 ] [0. 0.64554304 0. 0.31315398 0.99999994 0. 0. ] [0.20378576 0.08823138 0.37446705 0.60019135 0. 1. 0.1440983 ] [0. 0.10185669 0.1440983 0.4952337 0. 0.1440983 0.99999994]] 本文中的代码使用Jupyter Notebook编写，需要的朋友可以直接到github上查看。]]></content>
      <categories>
        <category>MachineLearning</category>
      </categories>
      <tags>
        <tag>gensim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优秀的程序员是什么样的？]]></title>
    <url>%2F2017%2F11%2F03%2Fdo-not-trust-user-input%2F</url>
    <content type="text"><![CDATA[在程序员的世界里有一句经典的至理名言：永远不要相信用户的输入。相信很多开发者都有过类似的踩坑经历，所谓“幸福的人都是相似的，而编程的人是幸福的”。 初入职时，作为一名后端工程师，每次实现接口后，老大(我的组长)都会过来帮我瞄一眼，然后提出一堆改进意见，其中最常被提到的一点就是：永远不要相信用户的输入。我不以为意，没怎么去改相关逻辑，这么做的后果就是老大下次再帮我检查时，总会伴随一阵悦耳的争辩声。 Round 1 你这里为什么不加个类型检查？万一接收到的值不是数组怎么办？ 怎么会呢，这些接口类型都是和前端定好的呀。 前端传入的数据是可以被篡改的，万一有人模拟了一个前端请求，发送一个其他类型的变量，那你的程序不就炸了？ 那样的话程序会返回服务器异常，攻击者也不会得到什么有用的信息吧… 是没什么损失，但为什么要给别人留下这么个漏洞呢？ 哦哦… Round 2 这个接口传入的是图片对吧，为什么不检查下图片的大小？ 这个，前端已经检查过了，太大的图片前端会先进行压缩的。 不是和你说过了吗，永远不要相信前端的输入！万一有攻击者模拟前端接口发来很大的图片呢？ 额，不是还有nginx可以挡一道吗？太大的图片应该直接无法响应吧。 nginx的确能做这个，但这些也是需要配置参数的，万一不小心参数设太大了没发现怎么办？ 哦，那就只能接收并保存下来了，不过我们的图片不都放在s3上吗，大一点也没什么关系吧，存储也挺便宜的… 你太天真了！首先，s3也是要花钱的，虽然空间不贵，但流量贵啊，万一真被人发现这个漏洞，他们完全可以把大量的图片通过你这里上传，然后根据返回值里的图片链接去获取。甚至如果有不怀好意的人传了些不该传的东西上去，那咱说不定都要去局里坐坐，这可不是开玩笑。 Game Over 这下我算彻底服了，只好满口答应，然后老老实实把该补的补上。嘴上虽这么说，但心里难免还有点疙瘩，这些都是特殊情况嘛，哪有那么容易就发生。事情总是这样，只要还没在自己身上发生，就会抱有侥幸心理，认为它一定不会发生，等到真的发生了，又来责怪自己当初怎么那么不小心。笔者就有过这么个经历，虽没犯下什么大错，但也足够长点记性了。 话说最近在做微信公众号开发，某天PM(产品经理)姐姐拿着个新需求过来，说是要加一个批量导入历史用户的功能，我扫了一眼开发文档，在用户管理文档中找到了获取用户基本信息这一项，心想不正是这个接口么，于是自信地回了句：“明天下班前给你”，转头就热火朝天地敲起了键盘。 整个流程比较清晰，先通过接口调用凭据access_token获取用户的openid列表，再调用获取用户基本信息接口来得到用户的昵称等信息，再将获取到的信息存到数据库即可。不过，作为一个严谨的开发，当然不能这么草率，还要考虑下效率和安全的问题。微信公众号里的用户动不动就上万，要一个个去获取基本信息那绝对是不妥当的，这一点微信团队的大佬们当然也想到了，所以提供了一个批量获取微信用户信息的接口，甚是方便。 另外，将数据存储到数据库时，当然也不能一条条地存入，这样一不小心得把数据库整挂了。应该在程序里构造sql语句，累计到一定的用户量之后再以批量的方式插入，这样不仅减少了操作数据库的次数，而且直接执行sql语句的方式也更加高效。 考虑完这两点后，就开始敲代码了。手指翻飞间，几个接口函数已经基本完成，打开编译器，一路绿灯，又麻利地打开本地的前端服务，准备开始调试。点击导入历史用户按钮，后端收到请求，调试器里查看，微信正确地返回了用户列表，因为线下的环境只是个测试号，所以只有20人左右，继续点击下一步，用户信息也成功获取，下一步，sql语句执行成功，全部用户导入完毕，到数据库里一看，数据确实都存下来了，搞定！ 接下来，就是将程序上线了。虽然只是个小功能，可一旦要上线都是需要找老大来的，毕竟万一线上的代码真出了问题这锅最后还得到他头上。于是我就跑去找来了他，他当然也知道我今天在开发这个功能，看我不到半天就完成了，还捎带夸了句：完成地挺快呵。我自然是轻扬起嘴角，淡淡地回了句：这活还蛮简单的。 然而，这种笑容很快就消失了，因为我看到坐在我电脑前的老大眉头紧缩，顿觉头顶乌云密布。果然，不到一会儿，就传来了老大的“盘问”。 你直接用sql语句来插入数据的？ 嗯，感觉这样子更方便一点，效率也更快点。 这个想法是没错，但你这里有个问题啊，在构造sql语句的时候，要特别注意单引号的使用，你看你这里在每个变量的两边显式地加入了单引号，那要是变量的字符串里本身就包含单引号怎么办呢？ 啊，变量的值怎么可能会有单引号啊，这一点我没想到诶。 怎么不可能？你这里的值是微信用户的信息对吧，万一哪个家伙无聊在昵称里加了个单引号呢，这完全是可能的，这种时候你的sql语句就会被这个单引号提前封闭，就会产生语法错误，也就无法正常地执行了。更要命的是，这个时候用户单引号后面的部分就成为了sql命令的一部分被执行了，万一来个drop database之类的，再加个单引号把原来的部分还原，那这就成了典型的注入攻击了。到时候你哭都来不及。 哇靠，原来还有这种操作啊，我如梦初醒，不觉后背发凉。之前听老大提起过在php时代著名的注入攻击的案例，没想到今天自己竟差点犯了这个错误，真是罪过。那该怎么解决呢？我向老大请教。 这个其实也挺简单，只要把可能存在的特殊字符给转义就行了，这样它们就不会变成sql的一部分被执行。 嗯，有道理。于是我就噼里啪啦改起来，不一会儿就把该转义的给转义了，老大看完微微点头，然后再次强调说，特别是涉及到数据库的操作时，一定要严格地检查用户的输入，考虑所有可能的情况，防止出现这样的问题。我毕恭毕敬地听着，点头如捣蒜。 不过事情还没完。老大紧接着又指出另一个问题：你在接收到微信服务器返回的用户信息列表时，有检查它的类型吗？ 我有点疑惑，这个类型不是在微信的文档里写好的嘛，只能是JSON啊，难道这也需要检查吗？ 老大貌似看出了我的疑虑，问到：万一用户的信息里确实有一些特殊的字符，没办法用JSON的方式传输呢？是不是需要先将JSON做个序列化再传输呢？而你没有判断返回值的类型，完全按照JSON的格式来处理，这样后面有可能会出错的。 我还是有点疑惑，不就是个昵称嘛，能有什么特别的字符呢？ 老大看了看我，建议我现在去试下微信修改昵称的功能。这不看还真不知道，在改昵称的时候原来可以插入表情的啊，还不止是微信官方提供的表情，而是能添加自己私藏的任意表情，表情是海量的，而JSON能识别的字符集是有限的，这么一来也就自然会出现一些JSON无法识别的字符。 看到这，我算是相信了，于是又把类型检查给加上了，并且当接收到的类型是字符串时，对其进行了处理，消除了里面可能存在的非JSON字符。老大看了看我，这次算是露出了比较满意的表情。 最后，又检查了一遍，为了验证刚才老大的两个想法，我还特意加了一个记录日志的操作，看看接收到的信息到底都是些啥。确认无误后，老大把程序更新到了线上。两三分钟后，更新完成，我迫不及待地尝试了下这个新功能。说来您还真别不信，线上用户的公众号总共有两万多关注者，这些人里面居然真的有人在昵称里使用了单引号，而且这类人还不少，大概三四百人里就有一个。而且返回的用户信息列表居然真的不全是JSON格式的，而是有部分string格式的，将这些string解析后得到的JSON里面，确实包含了一些无法识别的字符。 我看着日志里的结果，又看了看微信公众平台的官方文档，再回头看看老大那挂着浅浅笑容的脸颊，不禁肃然起敬。一方面是觉得尽管诸如微信这样的官方文档也难免会有疏漏之处，不可一味地根据主观意向来判断。再者，则是对老大的远见卓识佩服之至。我记得曾在知乎上看到过一个问题，大意是说 做一个优秀的程序员到底难在哪里？ 答案里有一条是这么写的： 由于你是一个优秀的（或仅仅是经验丰富的）程序员，你可以看出项目代码里存在着的隐患。你选择防患于未然，修复这些问题，但由于问题并没有真的发生，你所做的一切，在不那么优秀的程序员同事的眼中（以及老大眼中），看起来并没有什么产出。 诚哉斯言。回想起先前的经历，要不是老大及时指出代码里可能存在的错误，那肯定是要出问题的，虽然这一次不一定导致什么严重的后果，但若问题不除，迟早要吃大亏。此时老大已经起身准备离开了，他还有一大堆的事情要忙呢。不过在临走前，他又一次嘱咐了我：永远不要相信用户的输入。 嗯，这次我是真的记住了。望着老大远去的背影，我在心里默念到。 大概，这就是我心目中优秀程序员该有的样子。 参考链接 做一个优秀的程序员到底难在哪里？ - Van Bruce的回答 - 知乎]]></content>
      <categories>
        <category>idea</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[手把手教你入门微信公众号开发]]></title>
    <url>%2F2017%2F11%2F01%2Fbasic-wechat-develop%2F</url>
    <content type="text"><![CDATA[这是一篇关于微信公众号开发的入门指南，较为详细地介绍了开发微信公众号的基本原理，并且有相应的代码实现。如果您正打算要做公众号开发，却又苦于找不到一份简洁明了的入门级教学文档，那么这篇博客应该能解决您的问题。 缘起 近日腾讯发布2017年中报，报告中指出，腾讯二季度实现营业收入566.06亿元，经营盈利、期内盈利分别达到225.6亿元和182.54亿元。按照当前利润与员工数估算，腾讯员工的平均年薪达到80万… 尽管很多人吐槽王者荣耀里的小学生太坑爹，但不得不承认，近年来腾讯的变现能力确实强大地吓人。这之中微信和QQ作为用户的入口，起到了很大的作用。特别是微信，已然是一款装机必备的全民软件，如何借助这个庞大的平台搞点事情，分享下微信带来的用户红利，也就成为一件很值得研究的事。微信公众号是个人或者企业的一个宣传平台，通过开发微信公众号，可以给关注公众号的用户提供更多定制化的服务，进一步可以将服务转化为效益。本文旨在给正准备做微信公众号开发的朋友分享一点经验，从而尽快熟悉微信公众号开发的整体流程。在此基础上可以继续去学习一些高级的开发技巧（比如微信支付、账单系统之类的），让自己的公众号更加地精(zhi)美(qian)。 磨刀不误砍柴工微信公众号大家肯定都用过。目前微信公众号主要分为订阅号和服务号，每种账号又分为未认证和已认证，它们的差别主要在于具有不同的接口权限，下图（引用自微信开发实战系列）是一些例子： 总体来说，服务号权限 &gt; 订阅号权限，认证账号权限 &gt; 未认证账号权限。申请订阅号比较简单，服务号相对复杂点，另外要认证的话还要额外提交一些材料。我们可以根据不同的业务需求去申请不同类型的账号，基本上常用的权限列表已经可以满足大部分的场景。 开发微信公众号本质上和通常的网站开发并无区别。当我们进入一个公众号页面之后，我们可以向公众号发送文字、语音、图片等消息，也可以通过点击页面下方的菜单触发相应的功能。那么开发者与微信用户究竟是怎么进行交互的呢？实际上我们在公众号里的所有操作，都会发送到微信的服务器上，微信服务器将这些动作的具体含义按照一定的格式进行封装后，发送到微信公众号所对应的服务器上（这个服务器的地址可以由开发者在微信公众号的后台进行配置），开发者通过编写代码来处理不同的用户行为，并将处理后的结果按照一定的格式返回给微信服务器，再由微信服务器发送到微信公众号里面，从而完成了一次交互过程。在这里借用方倍老师博客中的一张图片来展示下这个过程，可以帮助大家理解地更清楚： 到这里我们可以知道，所谓的微信公众号开发，其实就是编写业务代码来处理用户的动作请求。这里面会涉及到和微信服务器之间的通信，也就涉及到一些安全认证方面的知识，后文会通过一个实际的例子进行说明。现在，就让我们来看看具体的流程吧。 巧妇难为无米之炊开发微信公众号需要准备以下两样食材： 微信公众号微信公众号可以在微信公众平台的官网上申请。前文说过，微信公众号分为几种类型，不同的类型具有不同的权限，具体的权限列表可以查看微信公众平台技术文档。值得注意的是，现在已经不再支持个人类型的公众账号申请微信认证。申请的过程无非是填写下邮箱和信息，建议使用QQ邮箱，毕竟是自家的东西。 服务器由于我们的服务器需要与微信服务器进行交互，因此必须能够让微信服务器可以访问到。很多公司都提供了云服务器租赁，价格不一，可以自行申请，细节在此不表。如果不想花钱申请，也可以使用一些外网穿透工具，将本地的IP暴露到公网中供外部访问，具体的工具请自行百度，不过大部分软件稳定性无法保证，而且分配的域名经常改变。个人建议还是申请一台服务器比较方便，等以后公众号运营良好开始涨(ying)粉(li)了，这些都不是事~ 撸起袖子加油干以下是详细步骤： 开启公众号开发者模式为了让微信服务器知道开发者服务器的存在，必须在公众号后台进行相应的配置。(1) 登录公众平台官网，找到左边功能栏的最下方，有一个基本配置的选项 (2) 点击基本配置按钮，在右边的页面中填写服务器的相关信息。其中URL填写http://外网IP:端口号/wx，这里外网IP是服务器的外网IP地址，端口号固定填写80。Token可以自由填写，用于两个服务器之间的验证。具体见下图： (3) 点击提交按钮，提示配置失败。这是自然的，因为我们还需要在开发者服务器上进行配置，才能完成验证的过程。 (4) 前面在配置微信公众号时为什么提示失败呢？在此我们有必要探究下这个认证过程。当我们点击了提交后，微信服务器会向我们所填写的那个URL发起一个GET请求，并携带以下几个参数：timestamp, nonce, echostr, signature。其中timestamp是一个时间戳，nonce是一个随机数，echostr也是随机数，这几个都很普通，重点在于signature，它的生成方式是将nonce、timestamp和token（也就是我们在网页中配置的TOKNE）三个字符串按照字典序排序后，对排序后得到的字符串数组使用哈希加密算法得到。我们的服务器在收到这个GET请求后，提取对应的参数，并按照前面说的方式生成hashcode，如果这个值与参数中的signature相同，那么我们就将echostr返回给微信服务器，否则返回空值。微信服务器收到这个echostr之后，验证这个值与它发送的echostr值是否相同，如果相同，说明这个值的确是由我们的服务器返回的，从而完成验证，今后所有的信息就都可以发送到这个服务器地址上。这里面涉及到了一些安全认证的相关知识，有兴趣的朋友可以去查阅更详细的资料。总的来说，就是让通信的双方都能够确认对方的真实身份。以下是认证部分的主要代码，使用Python2.7和web.py框架编写： 编写服务器业务逻辑前面我们完成了微信服务器与开发者服务器的相互认证过程，接下来我们需要编写业务逻辑代码来处理微信服务器发送过来的信息。以文本消息为例，当用户在公众号页面发送了消息后，微信服务器会将这条消息封装成如下的XML格式，并将其作为请求的内容向开发者服务器发起一个POST请求： 各个字段的具体含义就如字段名所示，比较直观。我们首先需要解析这个XML对象，并提取出各个字段用于后续的处理： 解析之后，我们可以在主函数中根据消息的不同类型，来调用不同的处理函数得到相应的处理结果，然后我们要将处理结果封装成同样的XML格式返回给微信服务器，封装XML对象的代码如下所示（以文本消息为例）： 至此，我们就完成了一个简单的回复流程（虽然目前这种只能回复文本消息==）。 更上一层楼以上就是一个基本的微信公众号开发流程。当然，想要让我们的公众号变得多姿多彩，需要掌握的内容还有很多。比如 (1) access_token：前面我们所做的实际是被动回复消息，微信服务器发起POST请求，我们将处理后的内容借由微信服务器返回给用户。如果我们需要主动地和用户进行交互，比如主动地向用户发一条消息，我们就需要调用微信公众平台提供的相应接口，并且需要主动告诉微信服务器我们的身份，这是通过access_token实现的。 access_token是微信公众号的全局唯一接口调用凭据，公众号在调用各个接口时都需要使用access_token。 如上所述，在我们调用各个接口前，需要先使用公众号的appid和appsecret信息（这两个值可以从微信公众号的网页上查看）向微信服务器请求获取access_token，然后带着这个值去调用微信公众平台提供的接口，实现相应的功能。 (2) 微信网页开发：网页开发就是指编写一系列的HTML5页面，并在微信公众号中引导用户打开我们开发的页面，从而实现相关的业务逻辑，这一功能使得公众号可以像一个内嵌在微信当中的应用一样，能够实现非常复杂的交互逻辑，而且相比于通常的应用来说更加小巧。 从这里出发以上就是本文的主体内容。楼主经验尚浅，斗胆提笔撰文，有不当之处欢迎各位指出。本文主要是一个入门的简介，后续的开发还有很多内容要学，以下列出一些个人认为比较不错的文档和资料，楼主也从中学到了很多，在此感谢各位作者。 (1) 微信公众平台技术文档(2) 方培工作室-微信公众平台开发教程(3) 微信开发者联盟(4) 微信开发实战系列 本文中的代码已上传到github，里面包含了微信公众号一些常用功能的实现，仅供参考：微信公众号开发示例程序]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>wechat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我们都曾行走在白夜里]]></title>
    <url>%2F2017%2F11%2F01%2Fbai-ye-xing-note%2F</url>
    <content type="text"><![CDATA[当我合上书本，再次看到封面上牵着手的小男孩和小女孩时，一种莫名的难过翻涌而来，投射到墙上的阴影仿佛要将整个房间笼罩。那里本该洒满阳光。 东野大叔的叙事风格宛若天马行空，这一点在《解忧杂货店》中就充分感受过，相比起来，《白夜行》可谓有过之而无不及，乃至于第一遍捧起书时到第五章就看不下去了，我已经忘了前面都说了啥。我是怀着一点猎奇的心来的，顶尖的推理小说作家，悬疑的杀人案件，如潮水般的好评，这足以让人坚信这本书能给人带来精彩的阅读体验。 但很显然，那时的我并没感受到。紧张的案件叙述部分在第一章的末尾戛然而止，接下来时光不断往前推进，各色人物一个接一个出现，发生的故事似乎没什么联系，那桩案子也宛如投向湖中的石子，随着波纹的远去逐渐淡出人们的视野。 难道就这样了吗？我不禁暗自揣测。合上书本，我瞅了眼封面，两个小孩子携手同行，他们的影子被拉得老长。这幅画有什么含义呢？我实在搞不懂。但东野大叔应该不会让人失望吧。 于是我又把书翻到了第一页。 东野大叔也确实没让人失望。 天网恢恢，疏而不漏。尽职的老刑警追了十九年，终于亲手将真凶绳之于法，而另一位主谋唐泽雪穗，则在失去唯一的精神支柱后彻底卸下了伪装。 她一次都没有回头。 这是全文的结尾，也是最精彩的地方之一。初看起来，这句话似乎进一步表现了雪穗的心狠手辣，对于和自己相依为命的伙伴的死也毫不在乎，但考虑到当时的场景，一个“陌生人”在自己店门口死亡，换做普通人应该会和她的店员们一样惊慌失措，而她却没有丝毫的情绪变化，这恰恰说明她此刻内心的波动之大，甚至让她忘记了身上一直披着的那层伪装。以这种方式接受惩罚，大概是最令人满意的结局。 东野大叔的确很擅长叙事，当看完全书再把前面的故事串起来时，才发现原来两位主角犯下了如此多的罪行，可谓千夫所指。这种人性的恶，在主人公身上被展现地淋漓尽致。 但整本书仅仅就是为了描述人性的恶吗？ 关于整个故事，有一个颇具争议的问题：该不该同情雪穗和亮司？ 我想，纵使不被同情，也不该把全部的错怪罪到他们身上。任何事情的发生都有个缘由。善恶终有来处。 出身贫苦，父亲早逝，与柔弱的母亲相依为命，被迫和陌生的大叔发生关系，这是唐泽雪穗的童年，也是导致其性格变化的直接原因。在那个破烂的潮湿的小屋子里，她度过了人生中本该最美好的一段时光，但留给她的却只有无尽的黑暗。母亲忙于生计，卑躬屈膝还要处处遭人白眼，连带着她一起受尽委屈。最可恨的是，为了生活，亲生母亲居然把她交易给胡子拉碴的大叔作为性工具，那是怎样一种无奈与痛苦。雪穗向母亲抗议过吗？我想大概是有的。但在母亲的恳求或是诱导下，她不得不接受这样的现实。从那一刻起，她的心里就已经埋下了邪恶的种子，随着时间的推移，慢慢生根发芽。 我从来就没有太阳，所以不怕失去。 正如她自已所说，她从一开始就是个身处黑暗的人，心中只剩欲望和仇恨。 桐原亮司的幼年也不快乐。虽然不愁吃穿，但风流成性的母亲和患有恋童癖的父亲显然没有给过他任何的关爱。他们不关心他每天都去了哪，干了啥，只管自己过得快活。他在家里没有任何人可以交流，于是他跑了出去，和同龄小朋友们玩耍是他少有的慰藉。直到他遇见了雪穗，他找到了一个可以说话的人，他把自己心爱的剪纸拿给她看，他和她经常相约在图书馆见面。多么美好的事啊，有这么可爱的玩伴。 他们本该这样长大。 当桐原拿起剪刀戳向父亲时，他一定花光了全部的力气。那种义无反顾，就像是去完成一件多么伟大的使命一般。只不过，此时的天空已被白夜笼罩。 从此枪虾和虾虎鱼开始了彼此依赖的生活。他们结伴而行，在白夜里摸索。 亮司最后强奸了美佳，变成了他父亲那样的人。雪穗安排了这一切，变成了她母亲那样的人。 这是小说中最具戏剧性的地方。受害者最终成为了害人者，新的受害人又是否会延续这样的轮回？循环往复，归途何处？ 每当我们在报道里看到，一些青年乃至少年犯下的极恶罪行，总是会习惯性地感叹一番人心的险恶，家教的缺失，然后再将《未成年人保护法》痛骂一番。罪犯固然可恨，该受到应有的制裁，但我们更应该考虑到，扭曲的心灵并非与生俱来的，黑暗的背后更需要阳光的照耀。 我曾认识这样一个女孩。 她出生在一个农村家庭，父亲嗜赌如命，在输光了家里所有的积蓄后，抛下她和母亲独自离去，那时她只有两岁半。母亲不堪重负，也狠心离去，将她留给了年迈的爷爷奶奶。村里的孩子经常欺负她，嘲笑她是个没人要的小孩，她很难过，又不知怎么反驳，就只是一味地哭。二老也没什么法子，就让她待在房间里，不准她出来。从此每天陪伴她的就只有洋娃娃和天花板，甚至连外面的蓝天白云都看不到。 后来她上了学，总是一个人坐在最角落的位置上，无心上课，成绩也一塌糊涂。后排调皮的男孩子经常拿她开玩笑，有的还动手动脚，她拼命想要反抗，但双拳难敌四手，她还是会经常受到欺负。终于有一天，她忍不住了，在放学路上捡起一块砖头狠狠扔向了其中一个人的头。 男孩受了重伤，家人找上门来，索要赔款，声称不赔钱就抓人。二老一边哭一边气，骂她是个没用的东西，和她爸一样，就知道干坏事。嘴上虽然骂娘，但钱还是得赔，总不能真让个十来岁的女娃子去蹲派出所。亲戚朋友们凑了点钱，把这事儿算是盖了过去。 后来倒也真平静了一阵儿，半年多没再闹事儿，估计是学校里的小伙儿们也心有戚戚，这姑娘惹不得。 那年春节，我爸妈觉得她家怪可怜的，又是街坊邻居的，就让我送点东西去她家。我把一篮子吃的提到厨房，和她奶奶聊了会儿家常。出门的时候，我不经意间回了个头，从旁边小房间里传来一道阴冷的光，差点没把我吓到。 她一个人蜷缩在房间的地板上，靠着床沿，边上是一台老旧的小彩电。她的目光朝我这边汇聚，眉头有一点点皱，紧咬着嘴唇，就那么死死地看着我。 我晃了晃神，匆忙地溜出大门，脚步飞快。 那是我一辈子忘不掉的眼神。 再后来，我去县城里上高中，少有机会回家，也就没再见到过她。只是某一年回去的时候听妈提起过，她好像很早就辍学了，离开了那个家，不知道去了哪。 天大地大，何处是家。 细想下这事儿，她的父亲肯定是要负首要责任的。父亲的恶习导致了家庭的破裂，给年幼的孩子造成了不可挽回的伤害。如果不想养家，又为何要成家？再往上找，父亲的恶习又是怎么造成的呢？大概是早年时不学无术，又没有一技之长，甚至懒惰地连活也不想干，于是就想找点快捷的方法。你说这该怪他的父母吗？可他们大概会说，我们要赚钱养家啊，哪有时间管孩子，只要他老老实实不给我们惹麻烦就行。他自己好赌，那是他天生的，这混账小子。 呵，养家糊口，不肖子孙，多么冠冕堂皇。 总有一些人，喜欢把孩子当做自己的附属品，只要给口饭吃给个衣服穿，就算是养他了。他们不懂得什么是教育，甚至不会教孩子基本的行为礼仪和道德廉耻，反正命是他的，爱咋咋地，老子当年也是这么过来的，不照样活得好好的？ 愚昧和无知，比黑暗本身更可怕。 想再讲个事儿。 近年来有个词在网络上出现的频率不断提升，就是“抑郁症”。 “某当红男星因抑郁症在家中服毒自杀，花样年华就此陨落。” “某中学女生因难忍同学的流言蜚语，终日恍惚不定，最后留下一纸遗书，从十楼的家中跳下。” 伴随着抑郁症而出现的，还有一种叫做“网络暴力”的现象。某人被爆“黑料”，全网通告，好事者火速赶往现场，不论是非曲直，群起而攻之，或图一时之嘴快，或蹭一时之流量，他们坚信法不责众，大家都说黑的肯定错不了。当事人还未晃过神，就要迎接漫天的声讨。理智者顺藤摸瓜，找准源头，可使烟消云散。更多的人则选择沉默，听之任之，不到数日，或在沉默中爆发，或在沉默中消亡。前者伤人，后者伤身，只留下一声叹息，以供凭吊。 而我们，或许就曾是众多好事者中的一个，尽管有时连我们自己可能都没意识到。 少一点刽子手，多一点摸摸头。我们没办法要求别人如何如何，但至少自己能做到不轻信，不盲从，凡事有个理性的判断，而非人云亦云。毕竟，思维能力是人区别于动物的最大特点，如果连独立思考都不会了，又与动物何异呢？ 愿白夜不再笼罩，黑夜里也能传来欢笑。]]></content>
      <categories>
        <category>idea</category>
      </categories>
      <tags>
        <tag>东野圭吾</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Mac上搭建基于Hexo的个人博客网站]]></title>
    <url>%2F2017%2F10%2F29%2Fcreate-hexo-blog%2F</url>
    <content type="text"><![CDATA[Hexo是一个快速、简洁且高效的博客框架，特别适合于部署静态的博客网站，更多介绍请移步官方文档。本文主要记录下笔者在Mac上实际的部署流程，包含期间遇到的坑和一些基本用法的介绍。 环境配置安装Hexo前需要先安装Git和Node.js 1. Git在Mac上安装Git最方便的方式是使用Homebrew，它是Mac上的一种包管理工具，能方便地安装和卸载软件。1$ brew install git 安装结束后在命令行输入git后运行，如果屏幕上出现git命令的使用帮助，说明安装成功。 在后文中可以看到，笔者借助github的网站作为个人博客网站的代理服务器。在进行博客网站部署时，会频繁涉及到本地与github网站的通信，为了免去每次通信时都要输入用户名和密码的烦恼，我们可以将本地的一组ssh-key传到github网站上，作为通信的凭据。 SSH是一种网络协议，全称Secure Shell，主要用于计算机之间的加密传输。以下介绍相应配置。 首先检查本地是否已存在ssh-keys1$ ls -a ~/.ssh/ 以上命令列出当前用户主目录下的.ssh目录中的所有文件，如果存在id_rsa和id_rsa.pub则跳过下一步，否则需要手动生成一组ssh-key 执行以下命令生成一组公/私钥，其中your_email@example.com需要替换成你自己的邮箱地址。1$ ssh-keygen -t rsa -C "your_email@example.com" 生成的文件默认会放在之前提到的.ssh目录中，我们需要获得公钥的值，在命令行输入1$ cat ~/.ssh/id_rsa.pub 该命令将公钥文件中的值，也就是一个很长的字符串输出到命令行中，复制该值备用。 接着登录github网站，单击右上角的头像，依次点击Settings -&gt; SSH and GPG keys -&gt; New SSH key 之后，会出现个文本框，在Title那一栏填写一些用于标识当前机器的信息，然后将前面复制的公钥值粘贴到Key那一栏，点击Add SSH key，即完成了添加操作，今后可以使用ssh地址直接从你的github仓库clone项目到本地，也可以将本地的修改直接push到github上，非常方便。 2. Node.jsNode是javascript的一种运行时环境，也是近年来非常流行的一门技术。安装Node.js的最佳方式是使用nvm。在命令行输入1$ curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.29.0/install.sh | bash 这条命令通过curl下载install.sh脚本，并执行脚本。待执行完成后，它会把nvm命令的执行路径放到~/.bashrc文件里，我们可以用cat命令来查看一下1234$ cat ~/.bashrcexport NVM_DIR="/Users/gao-yimei/.nvm"[ -s "$NVM_DIR/nvm.sh" ] &amp;&amp; . "$NVM_DIR/nvm.sh" # This loads nvm 确认存在以上配置后，通过执行source命令来使得环境变量生效1$ source ~/.bashrc 至此我们就把nvm安装好了，可以通过以下命令查看nvm版本号12$ nvm --version0.29.0 如果能正确打印出版本信息就证明nvm已经安装成功。接着通过执行以下命令来安装Node.js1$ nvm install stable 如果安装期间遇到权限问题，可以改用sudo的方式重新运行。 安装完成后，在命令行键入12$ node -vv7.1.0 如果能够正确打印出版本信息，则说明安装成功。 安装Hexo完成准备工作后，可以正式开始安装Hexo。在命令行输入1$ sudo npm install -g hexo-cli npm是Node.js自带的一个包管理工具，用于安装和卸载Node模块。上面的命令将Hexo这个模块以全局的方式安装到我们的电脑中，是Hexo官方推荐的方式。但笔者在运行这条命令时，出现了如下的错误12345678910111213141516npm ERR! Error: EACCES: permission denied, mkdir '/usr/local/lib/node_modules/hexo-cli'npm ERR! at Error (native)npm ERR! &#123; [Error: EACCES: permission denied, mkdir '/usr/local/lib/node_modules/hexo-cli']npm ERR! errno: -13,npm ERR! code: 'EACCES',npm ERR! syscall: 'mkdir',npm ERR! path: '/usr/local/lib/node_modules/hexo-cli',npm ERR! fstream_type: 'Directory',npm ERR! fstream_path: '/usr/local/lib/node_modules/hexo-cli',npm ERR! fstream_class: 'DirWriter',npm ERR! fstream_stack: npm ERR! [ '/usr/local/lib/node_modules/npm/node_modules/fstream/lib/dir-writer.js:35:25',npm ERR! '/usr/local/lib/node_modules/npm/node_modules/mkdirp/index.js:47:53',npm ERR! 'FSReqWrap.oncomplete (fs.js:82:15)' ] &#125;npm ERR! npm ERR! Please try running this command again as root/Administrator. 明明是以sudo方式运行的啊，为什么会出现权限错误呢？笔者一头雾水，最后还是在某度上找到了一个解决方案。执行以下代码1$ sudo npm install --unsafe-perm --verbose -g hexo 大意就是忽略一些安全方面的警告，强制安装。最终成功安装上了。 使用Hexo基本用法Hexo默认已经配置好了基本的选项，通过以下几步简单的操作就可以获得一个示例的博客网站。 终端切换到一个你准备用来搭建博客的目录，然后执行命令1$ hexo init myblog 其中myblog将会作为博客网站的本地主目录，该命令对文件夹进行了初始化，生成了一些建设网站所必须的材料。切换到myblog文件夹下，执行以下命令1$ npm install 前面提到过，npm install是用来安装Node.js模块的，当这个命令不带参数时，它将会读取当前目录(也就是myblog目录)下的package.json文件，并按照里面的描述来安装，所有的模块都会存在当前目录下一个名为node_modules的文件夹下。 等待安装完成后，仍在当前目录下，执行以下命令，开启Hexo服务器1$ hexo start 若一切正常，则命令行会打印出如下的提示信息12INFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 该信息表示Hexo服务器已经在本地的4000端口运行了。打开任意浏览器，在地址栏输入localhost:4000后回车，即可看到默认的欢迎页面。 修改配置默认页面上信息都是关于Hexo的，我们要将其改为自己的信息。在myblog目录下，使用ls命令查看该目录下的所有文件12$ ls_config.yml node_modules package.json scaffolds themes db.json package-lock.json public source yarn.lock 其中_config.yml保存了与网站相关的基本配置。使用vim打开，可以看到如下信息123456789101112# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: 你的博客名称subtitle:description: 你的博客介绍或者座右铭author: 你的昵称language: zh-Hanstimezone:... 最开始的部分就是关于网站的标题、作者和简介等信息，这里可以根据个人的情况填写，其中language那一栏填写zh-Hans表示网站采用简体中文。填写时注意各个字段的冒号后必须要空一格再填具体内容，填写完毕后保存即可。 接着我们来看一下Hexo中一项很实用也很强大的工具，那就是主题。大家之所以喜欢写个人博客，一个很重要的原因就是它的自由性，我们可以往博客上添加任何个人喜欢的元素，也可以根据自己的喜好来定制网站的背景、样式、配色等。Hexo为我们修改网站的风格提供了一种非常便捷的方法，只需要简单的几步，就可以将我们的网站修改成一种指定的风格，我们还可以自己定制喜欢的模板。下面以Hexo官网上提供的NexT主题为例，介绍下如何为我们的博客更换不同的主题。 在myblog目录下执行以下命令1$ git clone https://github.com/iissnan/hexo-theme-next themes/next Hexo中的主题实际上都是由不同的网友贡献的，以上命令将该主题所对应的github上的项目克隆到myblog目录的themes目录下。 接着打开myblog目录下的_config.yml文件，找到theme配置项，将对应的值改成next1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next 保存后退出。仍在myblog目录下，依次执行以下指令12$ hexo clean $ hexo generate 两条命令分别是清除缓存文件以及重新生成静态文件。重新启动Hexo服务，即可看到next主题风格的博客页面。 更多设置添加个人头像有了主题之后，我们可以在网站中设置侧边栏个人头像。在myblog目录下，打开themes/next/_config.yml文件，找到avatar配置项，将值设为头像的地址。该地址可以是一个完整的图片链接，也可以是一个站点内的相对路径。如果使用前者，为了保证图片链接的有效性，可以先将图片上传到github上再使用对应链接，而对于后者，则需要先将图片放到themes/next/source/images目录下，再使用相对路径引用。以下配置是一个采用相对路径的示例1234# Sidebar Avatar# in theme directory(source/images): /images/avatar.gif# in site directory(source/uploads): /uploads/avatar.gifavatar: /images/coolboy.jpg 添加标签Hexo默认只开启了两个标签：首页和归档，我们可以添加一些常用的其他标签，比如标签、分类和关于。先打开/themes/next/_config.yml文件，找到menu配置项，如下所示123456789101112131415161718menu: home: / || home #about: /about/ || user #tags: /tags/ || tags #categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat# Enable/Disable menu icons.menu_icons: enable: true home: home about: user categories: th tags: tags archives: archive 可以看到默认情况下只有home和archives前面没有#号，其他项都没注释掉了，因此我们要先把准备开启的标签前的#号给去掉，并且在下方menu_icons配置相应的值，这里配置的icon也就是网页上展现的对应的图标，实际上是由Font Awesomo网站提供的，涵盖了众多常用的网站图标。配好之后保存退出。 然后我们来创建需要的页面。首先是about页面，在myblog目录下运行命令1$ hexo new page "about" 这条命令会在source目录下新建一个名为about的文件夹，并生成一个index.md文件，使用任意的markdown编辑器打开后编辑，可以写上个人的介绍以及一些自己想说的话，完成后保存退出，这样一来about页面就做好了。 接下来是categories页面，同理，先在命令行运行1$ hexo new page categories 然后编辑刚刚创建的index.md文档，将页面类型设置为categories即可，Hexo会自动将带有不同类别标签的文章进行分类归档展示123456---title: Categoriesdate: 2017-10-27 17:27:06type: "categories"comments: false--- 这里还有个comments字段被设置为false，这是因为Hexo的博客可以外接一些第三方的评论系统，默认在所有页面都会显示，而我们一般只希望评论显示在文章主页面的下方，因此此处将评论给关闭。 最后是tags页面，同理先在命令行创建页面，然后编辑页面，将页面的类型设置为tags即可，Hexo的主题系统会自动地在这个页面中显示标签云1$ hexo new page "tags" 添加Read More按钮默认情况下我们的文章会在主页以全部展开的形式呈现，但我们通常希望在主页上每篇文章只显示一部分，这样既显得简洁，又可能让读者看到更多的文章，便于读者的判断。想要做到这一点很简单，只要在文章对应的markdown文档中，按照下图所示插入一句特殊的代码到我们想要进行截断的任意位置即可12345More info: [Server](https://hexo.io/docs/server.html)&lt;!--more--&gt;### Generate static files 这样首页上的文章只会显示之前的部分，然后会有一个阅读全文的按钮，点击之后再进入文章的主页面，看起来非常舒服，也很合理。 经过上面的这些设置后，我们就能得到一个看起来不错的博客网站了，可以运行Hexo服务，在浏览器上实时查看我们的网站，现在的情况就像这样 通过github发布现在我们已在把网站在本地搭起来了，为了让别人能通过互联网访问我们的网站，还需要将其发布到网络上，这里我们以发布到github上为例。 首先，我们要在github上新建一个Repository，名称固定为1yourNickName.github.io 其中yourNickName指的是你的github账户昵称，注意必须按照这个规则来命名。然后我们编辑myblog/_config.yml文件，在文件的最后几行找到deploy设置项，按照下面的方式进行修改123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: git@github.com:coolBoyGym/coolboygym.github.io.git branch: master 其中的repository字段需改成你自己的giuhub项目对应的地址。 修改完成后，在blog目录下依次运行以下命令12$ hexo generate$ hexo deploy Hexo会先在本地生成静态的网页文件，然后将所有文件推送到github上对应的项目中。一切正常的话，我们就可以通过下面的网址访问个人博客，应该能看到和本地一样的效果1https://yourNickName.github.io 到此，我们就算是真的搭建好一个简单的博客网站了，赶快请你的小伙伴们来看看吧~ 新建文章当我们想要写一篇新博客时，先执行以下命令1$ hexo new "My-New-Post" 其中My-New-Post是你想新建的博文的名字，这条指令会在myblog/source/_posts文件夹下新建一个名为My-New-Post.md的文件，我们可以使用任意的markdown编辑器来打开它进行编写，完成之后，我们可以先在本地查看下效果，确认无误后，同样通过之前的两条命令来将这篇新的博文发布到github上12$ hexo generate$ hexo deploy 关于markdown编辑器，笔者使用的是Cmd Markdown，界面比较简洁，可预览，功能也较全，值得推荐~ 以上就是本文的全部内容，涉及到了Hexo的一些基本操作，当然它的强大还远不止于此，通过集成各种第三方的服务，可以让我们的网站变得更加有趣，这个就有待读者们自己探究了，有什么好玩的第三方工具欢迎告诉我。 参考链接 与佳期的个人博客 Hexo NexT github/iissnan]]></content>
      <categories>
        <category>tool</category>
      </categories>
      <tags>
        <tag>Mac</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F10%2F28%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>daily</category>
      </categories>
  </entry>
</search>
